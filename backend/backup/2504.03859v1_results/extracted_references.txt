URL https://CRAN. R-project.org/package=ald. R package version 1.3.1. A. Signorell. DescTools: Tools for Descriptive Statistics, 2025. URL https://CRAN.R-project.org/package= DescTools. R package version 0.99.59. Ajit Agrawal, Suchita Sridhara, and Neeraj Sudhakar. Hit rate vs forward returns. https://akanomics.com/post/ hit-rate-vs-forward-returns , June 2023. Accessed: 2025-03-09. Michael Fleder and Devavrat Shah. Forecasting with alternative data. Proceedings of the ACM on Measurement and Analysis of

previous parameter estimates from our previous paper, we had a general idea of the range of the parameter estimates for the intercept w0 and scale parameters and note that these are still somewhat large ranges as our data is on the logarithmic scale. Lastly, 11 A PREPRINT - APRIL 8, 2025 we chose a Beta(2, 2) for since there was limited asymmetry in the residual plots from our previous paper. This choice gives slightly more weight when is closer to 0.5. Our Bayesian models utilized four chains with a

as BIAS = - true which we would expect to be nearly zero. The quantity A VG.SE= 1 N PN j=1 (j) as the average standard error of the estimates. The coverage COV = 1 N PN j=1 I(true {l(j), u(j)}) where l(j) and u(j) are the 0.025 and 0.975 quantiles of the posterior samples (j) 1 , . . . , (j) S providing the coverage of the 95% credible intervals for based on the dataset Dj. We should expect this value to be close to 0.95. If N is chosen sufficiently large we should expectMCSE 1 N PN j=1 (j) = A VG.SEand

van Eijk and Ghosh [2025] on revenue forecasting. However, exponential discounting could still be advantageous in other financial applications. The Stan software and its corresponding R interface, rstan, employ Hamiltonian Monte Carlo (HMC) methods to generate posterior samples given a specified log-likelihood function and log-prior density; for more details on HMC methods, see Neal et al. [2011]. This method contrasts with traditional Metropolis-Hastings (MH) algorithms used in software like JAGS. The

period followed by 10,000 samples for a total of S =20,000 post burn-in samples. We used the following uninformative priors: (i) w0 N(0, 1000), (ii) Dir(1, 1, 1, 1), (iii) InvGamma(2, 2), (iv) Beta(1, 1), and (v) InvGamma(2, 2). Initially, we used Inverse Gamma priors for the parameters and in our simulation study. However, following the recommended practices through documentation, we later switched to Half-Cauchy priors in Section 5 because they are often more well-behaved. Our simulation studies used the

predictive distributions for CRM. 25 A PREPRINT - APRIL 8, 2025 Figure 13: Posterior predictive distributions for CSCO. Figure 14: Posterior predictive distributions for IBM. 26 A PREPRINT - APRIL 8, 2025 Figure 15: Posterior predictive distributions for INTC. Figure 16: Posterior predictive distributions for INTU. 27 A PREPRINT - APRIL 8, 2025 Figure 17: Posterior predictive distributions for KLAC. Figure 18: Posterior predictive distributions for MSFT. 28 A PREPRINT - APRIL 8, 2025 Figure 19: Posterior

(j) = A VG.SEand the bias,BIAS = -true 0. The results for our posited models in (9), (10), and (11) are found in Table 1, Table 2, and Table 3 respectively. There are no issues with the results of our simulation studies. All the biases were nearly zero (i.e., no significant statistical biases), the average standard errors were close to the MCSE (indicating that we conducted a sufficient number of MC runs), and the coverages were around 0.95, though in a few cases, the COV estimates slightly exceeded the

hit and win rate losses, 2025. URL https://arxiv.org/abs/2503.20082. Roger Koenker and Gilbert Bassett Jr. Regression quantiles. Econometrica: journal of the Econometric Society, pages 33-50, 1978. Ryan Martin and Nicholas Syring. Direct gibbs posterior inference on risk minimizers: Construction, concentration, and calibration. In Handbook of Statistics, volume 47, pages 1-41. Elsevier, 2022. Victor Picheny, Henry Moss, Leonard Torossian, and Nicolas Durrande. Bayesian quantile and expectile optimisation.

distributed errors. For all the details and justifications, see van Eijk and Ghosh [2025]; however, we briefly recap the overall procedure here. We utilize quarterly revenue data from the Institutional Brokers' Estimate System (I/B/E/S) within the Wharton Research Data Services (WRDS). Our sample contains 23 technology companies across the years 2015 through 2023. The dataset includes the ground truth quarterly revenues with corresponding one-quarter-ahead analyst forecasts. Since an analyst can update

like JAGS. The workflow of Stan with HMC in rstan is (i) Define the Bayesian Model: The user specifies the likelihood function and prior distributions for parameters in the Stan modeling language; (ii) Automatic Log Posterior Gradient Computation: Stan internally computes the log posterior density by adding the log-likelihood, log-prior, and gradients; (iii) Gradient-Based Sampling with HMC: Rather than proposing random jumps like MH, HMC utilizes Hamiltonian dynamics, leveraging gradients of the log

Jags: A program for analysis of bayesian graphical models using gibbs sampling. In Proceedings of the 3rd international workshop on distributed statistical computing, volume 124, pages 1-10. Vienna, Austria, 2003. Bob Carpenter, Andrew Gelman, Matthew D Hoffman, Daniel Lee, Ben Goodrich, Michael Betancourt, Marcus Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell. Stan: A probabilistic programming language. Journal of statistical software, 76:1-32, 2017. Franz C Palm and Arnold Zellner. To combine or not

Figure 5: Posterior predictive distributions for ADBE. Figure 6: Posterior predictive distributions for ADI. 22 A PREPRINT - APRIL 8, 2025 Figure 7: Posterior predictive distributions for ADP. Figure 8: Posterior predictive distributions for AMAT. 23 A PREPRINT - APRIL 8, 2025 Figure 9: Posterior predictive distributions for AMD. Figure 10: Posterior predictive distributions for ANET. 24 A PREPRINT - APRIL 8, 2025 Figure 11: Posterior predictive distributions for A VGO. Figure 12: Posterior predictive

2). Figure 1 from Punzo and Bagnato [2024] shows that the parameter asymptotically falls within the range zero to four, which motivates us to use this range for the prior. The model in JAGS contained 10,000 samples for the burn-in followed by 20,000 samples. The trace plots produced by JAGS assessed convergence and no issues were present. Table 7 presents the results of our simulation study across {0.5, 1, 2}. One concern with this model is the slightly elevated bias when = 2, which further supports our

by computing the cumulative distribution functionF(; , ) = R - f(u; , )du as F(; , ) = ( 2G (1-) , if <= 0, + (1 - ) 2G - 1 , if >0 where, G(u) = Ru - g(z)dz is the cumulative distribution function for the density g(u). Thus, the median of is 1- G-1( 1 4 ) < 0 when >= 1 2 and the median is G-1(3-4 4-4 ) > 0 when <= 1 2 . Next, we provide the first three moments of the above density, assuming that g(u) has the corresponding finite moments. Lemma 2. Let the density f(; , ) as defined in Lemma 1 of a generic

shown in (9), (10), and (11), we define the prior distributions for the parameters as Dir(1, 1, . . . ,1), w0 N(0, 1), , Half-Cauchy(0, 1), Beta(2, 2). (12) See Section A of the appendix for definitions of the corresponding probability density functions. Setting all of the hyperparameters 1 = 2 = . . .= m = 1 for indicates a uniform distribution across all weights 1, 2, . . . , m because the Dir(1, 1, . . . ,1) distribution is uniform supported on the simplexSm. Next, based on our previous parameter

with Lin et al. [2022] that imputed missing values with the mean across all analysts for a given quarter. Our missing data model allows us to utilize the posterior predictive distribution (PPD) to impute any missing values given the observed data through Monte Carlo sampling. Ticker Asymmetric Laplace Asymmetric Normal Reverse Gumbel AAPL 83.3 83.3 83.3 ACN 87.5 87.5 87.5 ADBE 91.7 91.7 91.7 ADI 87.5 87.5 87.5 ADP 70.8 54.2 70.8 AMAT 87.5 87.5 87.5 AMD 79.2 79.2 79.2 ANET 95.8 100.0 100.0 A VGO 83.3 79.2

a sample size of n. Suppose (j) 1 , . . . , (j) S are post-burn-in MCMC samples based on the dataset Dj. We define the following quantities for performance evaluation: The posterior mean is estimated as (j) = 1 S PS s=1 (j) s . The posterior standard deviation is estimated as (j) = q 1 S-1 PS s=1((j) s - (j))2. The Monte Carlo standard error (MCSE) is q 1 N(N-1) PN j=1( (j) - )2, where = 1 N PN j=1 (j) is the sample average of posterior mean estimates. 9 A PREPRINT - APRIL 8, 2025 The bias as BIAS = - true

distribution with parameters 1 > 0, 2 > 0, . . . , m > 0, U(c0, d0) denotes a uniform distribution with lower bound c0 > 0 and upper bound d0 > c0, and InvGamma(a0, b0) denotes Inverse Gamma distribution with shape parameter a0 > 0 and scale parameter b0 > 0. Parameter BIAS A VG.SE MCSE COV 0.5 0.005 0.003 0.003 0.956 1.0 0.025 0.005 0.005 0.954 2.0 0.187 0.016 0.016 0.932 0.5 -0.010 0.006 0.006 0.952 1.0 -0.007 0.005 0.005 0.952 2.0 -0.040 0.006 0.006 0.934 w0 0.5 0.016 0.009 0.009 0.974 1.0 0.011 0.008

that m = 4 ). The observed data X consists of n x m = 100 x 4 observations that are random samples from a standard normal distribution. For both (9) and (10), we fix the scale parameter = 1 and run three simulation studies across the asymmetry parameter {0.25, 0.5, 0.75}. Whereas in (11), we ran three simulation studies across the scale/asymmetry parameter {1, 5, 10}. We utilized Stan via the R interface rstan. The specified models contained two chains with 5,000 samples for the burn-in period followed by

four chains with a burn-in of 5,000 samples followed by 10,000 samples for a total of 20,000 MCMC samples. Similar to Section 4, we used the built-in functions to check for numerical convergence and encountered no problems. Lastly, if an analyst j was missing a forecast for quarter t, we used the following missing data model: Xt,j N(t, 2 t ) where t and 2 t are the mean and variance across all analysts for a given quarter t. Using the cross-sectional estimates in revenue forecasting aligns with Lin et al.