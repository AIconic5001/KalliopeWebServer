being cited may have been published in any year (Scite's data contain publications from as early as the fifteenth century). As interest in various topics waxes and wanes, even older publications that are seminal works may receive a heightened level of interest in recent years. When institutional rankings are sorted by the weighted SI value (see Table 1), prominent names are surfaced, reflecting the high amount of publication output associated with research- intensive institutions. A similar pattern can be

writing, the Scite database contains over 1.4 billion citation statements - defined as the text surrounding a citation - from over 38 million separate publications. Scite ingests papers from a variety of sources, including open access publications and through indexing agreements with publishers. It then identifies the in-text citations and extracts the text surrounding it, and links it to the published work being referenced. Finally, the text of the citation - known as the citation statement - is

not just by the number of citations a given entity has received, but the content of the citations themselves. In addition to describing our methodology, we also provide an example of how it may be applied to three types of entities: institutions, journals, and fields. Using Scite to create a Content-Aware Metric The present work uses data from Scite, a large, curated collection of scholarly citations and their associated text (i.e., what one paper says about another). At the time of this writing, the Scite

either replacing or augmenting the raw reference count that is currently part of the SI equation. There are also additional, novel approaches to entity rankings that can be constructed by combining SI data with existing formulas. For example, the h-index can be easily modified to be based solely on supporting citations. An "integrity index" could be computed to highlight authors who routinely cite their previous work in a contrasting manner, indicating that they are open to revising their position on key

the type of citations an entity has received, we developed the Unweighted Scite Index (USI), defined simply as the number of supporting citations the entity has received divided by the sum of the supporting and contrasting citations it has received: This produces a ratio of supporting citation statements to all valanced citation statements - a value ranging 0-1, with larger values representing more support compared to fewer contrasting statements. This approach builds on previous work by Nicholson and

producing English-language research. Scite dataset of scholarly papers is similarly shaped by this global trend, but is addressing this problem step by step (e.g., Scite, 2022), gradually including sources in other scholarly languages. As noted earlier, while journal rankings are straightforward (given that our ability to link citations to specific works is quite good), institutional rankings are more complicated, as they rely on correctly identifying authors and tying them to institutions. Since author

impact that is based on the content of citations, we now turn to the computation of that metric and its application to entities - specifically, institutions, journals, and fields. This process involves aggregating locating citations over a given period, as well as tying those citations to the entity in question. For some entities (e.g., journals and fields), this linking task is straightforward, as the needed data is readily available in article metadata. For other entities - in particular, institutions -

as new data are generated with each iteration of ranking reports) will be especially informative and may warrant changes to how the Scite Index is computed. For example, changing how the unweighted SI is exponentiated is a straightforward way of adjusting how much importance is given to supporting and contrasting citations compared to references alone. Additionally, it may be useful to incorporate the third type of citation Scite generates - a mentioning citation - into the formula for SI, either replacing

institutions - this task is more complicated, as individual publications are tied to a given entity through authors, who may (a) have the same name as other authors, and (b) change institutions throughout their scholarly career. As such, authors must be disambiguated in order to be properly tied to a given institution. We should also note that, as other entities (e.g., countries, regions) are tied to papers through institutions, there is a large number of entities beyond institutions themselves that rely

with the most prominent metric of scholarly productivity, the Impact Factor (r = .03; see Figure 1). Figure 1: Relationship between Unweighted Scite Index (journals) and Impact Factor While the Unweighted Scite Index provides an accessible metric of replicability, it does not tell the full story, as only two of the three citation types Scite has identified are included (mentioning cites are not present). This results in entities receiving a small number of overall citations that happen to be supporting

& Research Academy 116 4,524 3 0.97 4.20 Figure 2: Institutional Scite Index scores by field Conclusions, Limitations, and Future Directions Using citations for institutional and journal rankings is a unique approach that results in meaningful differences when compared to other rankings reports. For example, CWTS Leiden Rankings measure scientific impact by examining the publication output of a curated list of universities. This metric represents productivity well but may be less effective in assessing

are susceptible to manipulation through practices like self-citation which inflate an institution's ranking without necessarily reflecting meaningful contributions to research (Buschman & Michalek, 2013). Surely, if publication records and citations are to meaningfully contribute to any institutional rankings scheme, the type of citations an institution receives is relevant. Here, we present a new approach to using citations in institutional rankings: one that measures scholarly output not just by the

(Erkkila & Piironen, 2020). A wide variety of factors - including data from surveys, sustainability indices, test scores, grant funding, and award receipts - are used in institutional rankings. One common indicator of success is research activity, which can be operationalized in many ways, including the number of papers and presentations linked to an institution through author affiliations. Such research output can itself be assessed by measuring the number of citations to works associated with an

(Fauzi et al., 2020; Marginson, 2007). This has led to calls for more comprehensive and transparent evaluation methods that take into account a wider range of academic activities and the broader societal impact of research (Adler & Harzing, 2009). The traditional reliance on bibliometric indicators such as citation counts to measure research activity also faces significant limitations. While counting citations associated with institutions or journals is a simple, intuitive way of measuring scholarly

statement - is classified into one of three types: supporting (e.g., "our results are consistent with," "we successfully replicate"), mentioning (e.g., statements related to theory, method, or implications; statements that do not directly address an empirical claim), or contrasting (e.g., "our findings do not support," "we failed to replicate"). Detailed information about how Scite ingests and classifies citation statements can be found in Nicholson et al. (2021). To aggregate and communicate the type of

that rely on author disambiguation. For rankings in the present paper, we address this issue by using a curated dataset for cited articles that has been vetted for accuracy: the Leiden 2023 Open Rankings data. This dataset uses the OpenAlex platform to link publications to institutions and has approximately 93% accuracy in author affiliations. For all three entity types, we examine citations in works published in the year 2024. This limitation is on the citing works only - the publications being cited may

Management, 29(2), 131-142. https://doi.org/10.1080/13600800701351660 Nicholson, J., & Lazebnik, Y. (2023). The R-Factor: A Measure of Scientific Veracity. Authorea Preprints. https://doi.org/10.15200/winn.140832.20404 Scite (2022). A Global Database of Citation Context and Coverage: Our Coverage in Turkey. Scite Blog. https://scite.ai/blog/2022-11-21_turkish_citation_coverage Selten, F., Neylon, C., Huang, C. K., & Groth, P. (2020). A longitudinal analysis of university rankings. Quantitative Science

all available open access content, author versions, as well as closed-access papers through indexing agreements with publishers, this approach does exclude closed-access papers from publishers with whom Scite does not have an indexing agreement. However, the weighted Scite Index score is based also on the number of references a paper has received - data that are often available as metadata, and thus obtainable regardless of whether an indexing agreement is in place. As the number of publishers with whom

position on key theories and/or findings. In short, we believe there is a bright future for a system of rankings - as well as other scholarly metrics - that incorporates the content of citations rather than there mere existence. References Adler, N. J., & Harzing, A.-W. (2009). When Knowledge Wins: Transcending the Sense and Nonsense of Academic Rankings. Academy of Management Learning & Education, 8(1), 72-95. doi:10.5465/amle.2009.37012181 Bellantuono, L., Monaco, A., Amoroso, N., Aquaro, V., Bardoscia,

a penalty to entities with lower USI scores - the extent of the penalty being determined by the exponent applied to the USI. This representation therefore incorporates the totality of information on an entity, but is less easily interpreted, as it has an undefined range. Therefore, we report both the weighted and unweighted SI values in order to provide data that is both easily interpretable and complete. Creating Rankings using USI and SI Having established a method of measuring scholarly impact that is